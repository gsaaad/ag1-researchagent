# Copyright (c) 2025 Marco Fago
# https://www.linkedin.com/in/marco-fago/
#
# This code is licensed under the MIT License.
# See the LICENSE file in the repository for the full license text.
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough,RunnableBranch
from dotenv import load_dotenv
load_dotenv()
# --- Configuration ---
# Ensure your API key environment variable is set (e.g.,GOOGLE_API_KEY)
try:
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash",temperature=0)
    print(f"Language model initialized: {llm.model}")
except Exception as e:
    print(f"Error initializing language model: {e}")
    llm = None
    # --- Define Simulated Sub-Agent Handlers (equivalent to ADKsub_agents) ---
def booking_handler(request: str) -> str:
    """Simulates the Booking Agent handling a request."""
    print("\n--- DELEGATING TO BOOKING HANDLER ---")
    return f"Booking Handler processed request: '{request}'. Result:Simulated booking action."
def info_handler(request: str) -> str:
    """Simulates the Info Agent handling a request."""
    print("\n--- DELEGATING TO INFO HANDLER ---")
    return f"Info Handler processed request: '{request}'. Result:Simulated information retrieval."
def unclear_handler(request: str) -> str:
    """Handles requests that couldn't be delegated."""
    print("\n--- HANDLING UNCLEAR REQUEST ---")
    return f"Coordinator could not delegate request: '{request}'. Please clarify."
    # --- Define Coordinator Router Chain (equivalent to ADK coordinator's instruction) ---
    # This chain decides which handler to delegate to.
coordinator_router_prompt = ChatPromptTemplate.from_messages([("system", """Analyze the user's request and determine which
    specialist handler should process it.
    - If the request is related to booking flights or hotels,
    output 'booker'.
    - For all other general information questions, output 'info'.
    - If the request is unclear or doesn't fit either category,
    output 'unclear'.
    ONLY output one word: 'booker', 'info', or 'unclear'."""),
    ("user", "{request}")
    ])
if llm:
    coordinator_router_chain = coordinator_router_prompt | llm | StrOutputParser()
    # --- Define the Delegation Logic (equivalent to ADK's Auto-Flow based on sub_agents) ---
    # Use RunnableBranch to route based on the router chain's output.
    # Define the branches for the RunnableBranch
    branches = {"booker": RunnablePassthrough.assign(output=lambda x:booking_handler(x['request']['request'])),"info": RunnablePassthrough.assign(output=lambda x:info_handler(x['request']['request'])),"unclear": RunnablePassthrough.assign(output=lambda x:unclear_handler(x['request']['request'])),}
    # Create the RunnableBranch. It takes the output of the router chain
    # and routes the original input ('request') to the corresponding handler
    delegation_branch = RunnableBranch((lambda x: x['decision'].strip() == 'booker', branches["booker"]),(lambda x: x['decision'].strip() == 'info', branches["info"]),branches["unclear"])
    # Combine the router chain and the delegation branch into a single runnable
    # The router chain's output ('decision') is passed along with the original input ('request')
    # to the delegation_branch.
    coordinator_agent = {"decision": coordinator_router_chain,"request": RunnablePassthrough()} | delegation_branch | (lambda x: x['output']) # Extract the final output
    # --- Example Usage ---
def main():
    if not llm:
        print("\nSkipping execution due to LLM initialization failure.")

        return
    print("--- Running with a booking request ---")
    request_a = "Book me a flight to London."
    result_a = coordinator_agent.invoke({"request": request_a})
    print(f"Final Result A: {result_a}")

    print("\n--- Running with an info request ---")
    request_b = "What is the capital of Italy?"
    result_b = coordinator_agent.invoke({"request": request_b})
    print(f"Final Result B: {result_b}")
    print("\n--- Running with an unclear request ---")
    request_c = "Tell me about quantum physics."
    result_c = coordinator_agent.invoke({"request": request_c})
    print(f"Final Result C: {result_c}")
if __name__ == "__main__":
    main()


# explain the code
# Script explained in 5 steps
# Load environment + initialize the LLM

# load_dotenv() loads your GOOGLE_API_KEY from .env.
# The script tries to create ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0). If it fails, it sets llm = None and later skips execution.
# Define “sub-agent” handler functions (simulated agents)

# booking_handler(request) simulates a booking specialist.
# info_handler(request) simulates a general info specialist.
# unclear_handler(request) handles requests that can’t be confidently routed.
# These are not LLM calls; they’re regular Python functions standing in for tools/agents.
# Create a router prompt to classify the user request

# coordinator_router_prompt is a system+user prompt that instructs the model to output only one word: booker, info, or unclear.
# This is the “coordinator” decision step.
# Build a routing + delegation pipeline (LCEL runnables)

# coordinator_router_chain = coordinator_router_prompt | llm | StrOutputParser() produces the routing decision as a plain string.
# RunnableBranch(...) checks that decision and routes to the corresponding handler branch.
# coordinator_agent = {"decision": coordinator_router_chain, "request": RunnablePassthrough()} | delegation_branch | (lambda x: x["output"])
# Runs the router and preserves the original request
# Delegates to the right handler
# Extracts the final handler output
# Run three example requests

# Booking example: “Book me a flight to London.” → should route to booking_handler
# Info example: “What is the capital of Italy?” → should route to info_handler
# Unclear example: “Tell me about quantum physics.” → based on the router rules, this likely goes to info (since it’s “all other general information”) unless the model outputs unclear.
# What the script is teaching the user
# Coordinator / router pattern: use an LLM to classify intent and then delegate to specialized handlers.
# Multi-agent decomposition without complexity: “sub-agents” can just be Python functions (or later: tools, APIs, real agents).
# LangChain LCEL routing primitives: how RunnablePassthrough and RunnableBranch let you build conditional flows.
# Structured decision outputs: enforcing a strict one-token/one-word decision (booker|info|unclear) makes routing reliable.
# Fail-safe execution: basic guard (if llm: / skip main) so the script doesn’t crash when credentials/model init fails.